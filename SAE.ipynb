{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "FgW523C7mwLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import torch\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer"
      ],
      "metadata": {
        "id": "hWvH7kADmuf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_file(dir: str, file_name: str) -> None:\n",
        "    if not os.path.isdir(dir):\n",
        "        raise ValueError(f\"The specified directory '{dir}' does not exist.\")\n",
        "\n",
        "    file_path = os.path.join(dir, file_name)\n",
        "    try:\n",
        "        open(file_path, \"x\").close()\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "\n",
        "\n",
        "def get_layer_activations(\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    plm: PreTrainedModel,\n",
        "    seqs: list[str],\n",
        "    layer: int,\n",
        "    device: Optional[torch.device] = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Get the activations of a specific layer in a pLM model. Let:\n",
        "\n",
        "    ```\n",
        "    N = len(seqs)\n",
        "    L = max(len(seq) for seq in seqs) + 2 # +2 for BOS and EOS tokens\n",
        "    D_MODEL = the layer dimension of the pLM, i.e. \"Embedding Dim\" column here\n",
        "        https://github.com/facebookresearch/esm/tree/main?tab=readme-ov-file#available-models\n",
        "    ```\n",
        "\n",
        "    The output tensor is of shape (N, L, D_MODEL)\n",
        "\n",
        "    Args:\n",
        "        tokenizer: The tokenizer to use.\n",
        "        plm: The pLM model to get the activations from.\n",
        "        seqs: The sequences to get the activations for.\n",
        "        layer: The layer to get the activations from.\n",
        "        device: The device to use.\n",
        "\n",
        "    Returns:\n",
        "        The (N, L, D_MODEL) activations of the specified layer.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    inputs = tokenizer(seqs, padding=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = plm(**inputs, output_hidden_states=True)\n",
        "    layer_acts = outputs.hidden_states[layer]\n",
        "    del outputs\n",
        "    return layer_acts\n",
        "\n",
        "\n",
        "def train_val_test_split(\n",
        "    df: pl.DataFrame, train_frac: float = 0.9\n",
        ") -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split the sequences into training, validation, and test sets. train_frac specifies\n",
        "    the fraction of examples to use for training; the rest is split evenly between\n",
        "    validation and test.\n",
        "\n",
        "    Doing this by samples so it's stochastic.\n",
        "\n",
        "    Args:\n",
        "        seqs: The sequences to split.\n",
        "        train_frac: The fraction of examples to use for training.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    is_train = pl.Series(\n",
        "        np.random.choice([True, False], size=len(df), p=[train_frac, 1 - train_frac])\n",
        "    )\n",
        "    seqs_train = df.filter(is_train)\n",
        "    seqs_val_test = df.filter(~is_train)\n",
        "\n",
        "    is_val = pl.Series(np.random.choice([True, False], size=len(seqs_val_test), p=[0.1, 0.9]))\n",
        "    seqs_val = seqs_val_test.filter(is_val)\n",
        "    seqs_test = seqs_val_test.filter(~is_val)\n",
        "    return seqs_train, seqs_val, seqs_test\n",
        "\n",
        "\n",
        "def parse_swissprot_annotation(annotation_str: str, header: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Parse a SwissProt annotation string like this:\n",
        "\n",
        "    ```\n",
        "    MOTIF 119..132; /note=\"JAMM motif\"; /evidence=\"ECO:0000255|PROSITE-ProRule:PRU01182\"\n",
        "    ```\n",
        "    where MOTIF is the header argument.\n",
        "\n",
        "    Returns:\n",
        "        [{\n",
        "            \"start\": 119,\n",
        "            \"end\": 132,\n",
        "            \"note\": \"JAMM motif\",\n",
        "            \"evidence\": \"ECO:0000255|PROSITE-ProRule:PRU01182\",\n",
        "        }]\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    occurrences = [o for o in annotation_str.split(header + \" \") if len(o) > 0]\n",
        "    for o in occurrences:\n",
        "        parts = [p for p in o.split(\"; /\") if len(p) > 0]\n",
        "\n",
        "        pos_part = parts[0]\n",
        "        coords = pos_part.split(\"..\")\n",
        "\n",
        "        annotations_dict = {}\n",
        "        for part in parts[1:]:\n",
        "            key, value = part.split(\"=\", 1)\n",
        "            annotations_dict[key] = value.replace('\"', \"\").replace(\";\", \"\").strip()\n",
        "\n",
        "        try:\n",
        "            list(map(int, coords))\n",
        "        except ValueError:\n",
        "            continue\n",
        "        if len(annotations_dict) == 0:\n",
        "            continue\n",
        "\n",
        "        res.append(\n",
        "            {\n",
        "                \"start\": int(coords[0]),\n",
        "                \"end\": int(coords[1]) if len(coords) > 1 else int(coords[0]),\n",
        "                **annotations_dict,\n",
        "            }\n",
        "        )\n",
        "    return res"
      ],
      "metadata": {
        "id": "seorEmrxm6kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SAE Model"
      ],
      "metadata": {
        "id": "HZMtsjGXmzyL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2suV2Z2Se1tp"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        d_hidden: int,\n",
        "        k: int = 128,\n",
        "        auxk: int = 256,\n",
        "        batch_size: int = 256,\n",
        "        dead_steps_threshold: int = 2000,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the Sparse Autoencoder.\n",
        "\n",
        "        Args:\n",
        "            d_model: Dimension of the pLM model.\n",
        "            d_hidden: Dimension of the SAE hidden layer.\n",
        "            k: Number of top-k activations to keep.\n",
        "            auxk: Number of auxiliary activations.\n",
        "            dead_steps_threshold: How many examples of inactivation before we consider\n",
        "                a hidden dim dead.\n",
        "\n",
        "        Adapted from https://github.com/tylercosgrove/sparse-autoencoder-mistral7b/blob/main/sae.py\n",
        "        based on 'Scaling and evaluating sparse autoencoders' (Gao et al. 2024) https://arxiv.org/pdf/2406.04093\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_enc = nn.Parameter(torch.empty(d_model, d_hidden))\n",
        "        self.w_dec = nn.Parameter(torch.empty(d_hidden, d_model))\n",
        "\n",
        "        self.b_enc = nn.Parameter(torch.zeros(d_hidden))\n",
        "        self.b_pre = nn.Parameter(torch.zeros(d_model))\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        self.k = k\n",
        "        self.auxk = auxk\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.dead_steps_threshold = dead_steps_threshold / batch_size\n",
        "\n",
        "        # TODO: Revisit to see if this is the best way to initialize\n",
        "        nn.init.kaiming_uniform_(self.w_enc, a=math.sqrt(5))\n",
        "        self.w_dec.data = self.w_enc.data.T.clone()\n",
        "        self.w_dec.data /= self.w_dec.data.norm(dim=0)\n",
        "\n",
        "        # Initialize dead neuron tracking. For each hidden dimension, save the\n",
        "        # index of the example at which it was last activated.\n",
        "        self.register_buffer(\"stats_last_nonzero\", torch.zeros(d_hidden, dtype=torch.long))\n",
        "\n",
        "    def topK_activation(self, x: torch.Tensor, k: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply top-k activation to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to apply top-k activation on.\n",
        "            k: Number of top activations to keep.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor with only the top k activations preserved,and others\n",
        "            set to zero.\n",
        "\n",
        "        This function performs the following steps:\n",
        "        1. Find the top k values and their indices in the input tensor.\n",
        "        2. Apply ReLU activation to these top k values.\n",
        "        3. Create a new tensor of zeros with the same shape as the input.\n",
        "        4. Scatter the activated top k values back into their original positions.\n",
        "        \"\"\"\n",
        "        topk = torch.topk(x, k=k, dim=-1, sorted=False)\n",
        "        values = F.relu(topk.values)\n",
        "        result = torch.zeros_like(x)\n",
        "        result.scatter_(-1, topk.indices, values)\n",
        "        return result\n",
        "\n",
        "    def LN(\n",
        "        self, x: torch.Tensor, eps: float = 1e-5\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Apply Layer Normalization to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor to be normalized.\n",
        "            eps: A small value added to the denominator for numerical stability.\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "                - The normalized tensor.\n",
        "                - The mean of the input tensor.\n",
        "                - The standard deviation of the input tensor.\n",
        "\n",
        "        TODO: Is eps = 1e-5 the best value?\n",
        "        \"\"\"\n",
        "        mu = x.mean(dim=-1, keepdim=True)\n",
        "        x = x - mu\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        x = x / (std + eps)\n",
        "        return x, mu, std\n",
        "\n",
        "    def auxk_mask_fn(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Create a mask for dead neurons.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A boolean tensor of shape (D_HIDDEN,) where True indicates\n",
        "                a dead neuron.\n",
        "        \"\"\"\n",
        "        dead_mask = self.stats_last_nonzero > self.dead_steps_threshold\n",
        "        return dead_mask\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the Sparse Autoencoder. If there are dead neurons, compute the\n",
        "        reconstruction using the AUXK auxiliary hidden dims as well.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "                - The reconstructed activations via top K hidden dims.\n",
        "                - If there are dead neurons, the auxiliary activations via top AUXK\n",
        "                    hidden dims; otherwise, None.\n",
        "                - The number of dead neurons.\n",
        "        \"\"\"\n",
        "        x, mu, std = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "\n",
        "        pre_acts = x @ self.w_enc + self.b_enc\n",
        "\n",
        "        # latents: (BATCH_SIZE, D_EMBED, D_HIDDEN)\n",
        "        latents = self.topK_activation(pre_acts, k=self.k)\n",
        "\n",
        "        # `(latents == 0)` creates a boolean tensor element-wise from `latents`.\n",
        "        # `.all(dim=(0, 1))` preserves D_HIDDEN and does the boolean `all`\n",
        "        # operation across BATCH_SIZE and D_EMBED. Finally, `.long()` turns\n",
        "        # it into a vector of 0s and 1s of length D_HIDDEN.\n",
        "        #\n",
        "        # self.stats_last_nonzero is a vector of length D_HIDDEN. Doing\n",
        "        # `*=` with `M = (latents == 0).all(dim=(0, 1)).long()` has the effect\n",
        "        # of: if M[i] = 0, self.stats_last_nonzero[i] is cleared to 0, and then\n",
        "        # immediately incremented; if M[i] = 1, self.stats_last_nonzero[i] is\n",
        "        # unchanged. self.stats_last_nonzero[i] means \"for how many consecutive\n",
        "        # iterations has hidden dim i been zero\".\n",
        "        self.stats_last_nonzero *= (latents == 0).all(dim=(0, 1)).long()\n",
        "        self.stats_last_nonzero += 1\n",
        "\n",
        "        dead_mask = self.auxk_mask_fn()\n",
        "        num_dead = dead_mask.sum().item()\n",
        "\n",
        "        recons = latents @ self.w_dec + self.b_pre\n",
        "        recons = recons * std + mu\n",
        "\n",
        "        if num_dead > 0:\n",
        "            k_aux = min(x.shape[-1] // 2, num_dead)\n",
        "\n",
        "            auxk_latents = torch.where(dead_mask[None], pre_acts, -torch.inf)\n",
        "            auxk_acts = self.topK_activation(auxk_latents, k=k_aux)\n",
        "\n",
        "            auxk = auxk_acts @ self.w_dec + self.b_pre\n",
        "            auxk = auxk * std + mu\n",
        "        else:\n",
        "            auxk = None\n",
        "\n",
        "        return recons, auxk, num_dead\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward_val(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the Sparse Autoencoder for validation.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The reconstructed activations via top K hidden dims.\n",
        "        \"\"\"\n",
        "        x, mu, std = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "        pre_acts = x @ self.w_enc + self.b_enc\n",
        "        latents = self.topK_activation(pre_acts, self.k)\n",
        "\n",
        "        recons = latents @ self.w_dec + self.b_pre\n",
        "        recons = recons * std + mu\n",
        "        return recons\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def norm_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Normalize the weights of the Sparse Autoencoder.\n",
        "        \"\"\"\n",
        "        self.w_dec.data /= self.w_dec.data.norm(dim=0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def norm_grad(self) -> None:\n",
        "        \"\"\"\n",
        "        Normalize the gradient of the weights of the Sparse Autoencoder.\n",
        "        \"\"\"\n",
        "        dot_products = torch.sum(self.w_dec.data * self.w_dec.grad, dim=0)\n",
        "        self.w_dec.grad.sub_(self.w_dec.data * dot_products.unsqueeze(0))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_acts(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Get the activations of the Sparse Autoencoder.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The activations of the Sparse Autoencoder.\n",
        "        \"\"\"\n",
        "        x, _, _ = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "        pre_acts = x @ self.w_enc + self.b_enc\n",
        "        latents = self.topK_activation(pre_acts, self.k)\n",
        "        return latents\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x, mu, std = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "        acts = x @ self.w_enc + self.b_enc\n",
        "        return acts, mu, std\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def decode(self, acts: torch.Tensor, mu: torch.Tensor, std: torch.Tensor) -> torch.Tensor:\n",
        "        latents = self.topK_activation(acts, self.k)\n",
        "\n",
        "        recons = latents @ self.w_dec + self.b_pre\n",
        "        recons = recons * std + mu\n",
        "        return recons\n",
        "\n",
        "\n",
        "def loss_fn(\n",
        "    x: torch.Tensor, recons: torch.Tensor, auxk: Optional[torch.Tensor] = None\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Compute the loss function for the Sparse Autoencoder.\n",
        "\n",
        "    Args:\n",
        "        x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "        recons: (BATCH_SIZE, D_EMBED, D_MODEL) reconstructed activations via top K\n",
        "            hidden dims.\n",
        "        auxk: (BATCH_SIZE, D_EMBED, D_MODEL) auxiliary activations via top AUXK\n",
        "            hidden dims. See A.2. in https://arxiv.org/pdf/2406.04093.\n",
        "\n",
        "    Returns:\n",
        "        tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "            - The MSE loss.\n",
        "            - The auxiliary loss.\n",
        "    \"\"\"\n",
        "    mse_scale = 1\n",
        "    auxk_coeff = 1.0 / 32.0  # TODO: Is this the best coefficient?\n",
        "\n",
        "    mse_loss = mse_scale * F.mse_loss(recons, x)\n",
        "    if auxk is not None:\n",
        "        auxk_loss = auxk_coeff * F.mse_loss(auxk, x - recons).nan_to_num(0)\n",
        "    else:\n",
        "        auxk_loss = torch.tensor(0.0)\n",
        "    return mse_loss, auxk_loss\n",
        "\n",
        "\n",
        "def estimate_loss(\n",
        "    plm: PreTrainedModel,\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    layer: int,\n",
        "    sae_model: SparseAutoencoder,\n",
        "    examples_set: pl.DataFrame,\n",
        "    sample_size: int = 100,\n",
        "):\n",
        "    \"\"\"\n",
        "    Estimate the loss of the Sparse Autoencoder using a set of examples.\n",
        "\n",
        "    Args:\n",
        "        sae_model: The Sparse Autoencoder model.\n",
        "        examples_set: The examples set to estimate the loss on.\n",
        "        sample_size: The number of examples to sample.\n",
        "\n",
        "    Returns:\n",
        "        float: The estimated loss.\n",
        "    \"\"\"\n",
        "    samples = examples_set.sample(sample_size)\n",
        "    test_losses = []\n",
        "    seqs = [row[\"Sequence\"] for row in samples.iter_rows(named=True)]\n",
        "    layer_acts = get_layer_activations(tokenizer=tokenizer, plm=plm, seqs=seqs, layer=layer)\n",
        "\n",
        "    recons = sae_model.forward_val(layer_acts)\n",
        "    mse_loss, _ = loss_fn(layer_acts, recons)\n",
        "    test_losses.append(mse_loss.item())\n",
        "\n",
        "    del layer_acts\n",
        "    return np.mean(test_losses)"
      ],
      "metadata": {
        "id": "lFa4MgSRnAzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "IamEPGVSfhPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "#fix for numpy version 2.0+\n",
        "np.float_ = np.float64\n",
        "np.complex_ = np.complex128\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import wandb\n",
        "from data_module import SequenceDataModule\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from sae_module import SAELightningModule"
      ],
      "metadata": {
        "id": "3zn8oDSnnKf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d95a27f3-8e51-490f-a347-7ef44f020d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39mcomplex_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcomplex128\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequenceDataModule\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n",
            "File \u001b[0;32m~/.conda/envs/jupyter_env_311/lib/python3.11/site-packages/wandb/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# This needs to be early as other modules call it.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m termsetup, termlog, termerror, termwarn\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     30\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwandb_lib \u001b[38;5;241m=\u001b[39m wandb_sdk\u001b[38;5;241m.\u001b[39mlib\n",
            "File \u001b[0;32m~/.conda/envs/jupyter_env_311/lib/python3.11/site-packages/wandb/sdk/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_artifacts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Artifact  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_init\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _attach, init  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_login\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m login  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_require\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/jupyter_env_311/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmagic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m magic_install\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_artifact_representation, sentry_exc\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_login, wandb_setup\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Backend\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filesystem, ipython, module, reporting, telemetry\n",
            "File \u001b[0;32m~/.conda/envs/jupyter_env_311/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apikey\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_settings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings, Source\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InternalApi\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_host_wandb_setting\u001b[39m(host: Optional[\u001b[38;5;28mstr\u001b[39m], cloud: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.conda/envs/jupyter_env_311/lib/python3.11/site-packages/wandb/sdk/wandb_settings.py:39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UsageError\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_setup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _EarlyLogger\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apikey\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GitRepo\n",
            "File \u001b[0;32m~/.conda/envs/jupyter_env_311/lib/python3.11/site-packages/wandb/sdk/wandb_setup.py:28\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     Any,\n\u001b[1;32m     20\u001b[0m     Dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     Union,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_manager\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_settings\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_util, server, tracelog\n",
            "File \u001b[0;32m~/.conda/envs/jupyter_env_311/lib/python3.11/site-packages/wandb/sdk/wandb_manager.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexit_hooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExitHooks\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_hooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unregister_all_post_import_hooks\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m settings_dict_from_pbmap\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n",
            "File \u001b[0;32m~/.conda/envs/jupyter_env_311/lib/python3.11/site-packages/wandb/sdk/lib/proto_util.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_internal_pb2 \u001b[38;5;28;01mas\u001b[39;00m pb\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_server_pb2 \u001b[38;5;28;01mas\u001b[39;00m spb\n",
            "File \u001b[0;32m~/.conda/envs/jupyter_env_311/lib/python3.11/site-packages/wandb/proto/wandb_internal_pb2.py:15\u001b[0m\n\u001b[1;32m     11\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timestamp_pb2 \u001b[38;5;28;01mas\u001b[39;00m google_dot_protobuf_dot_timestamp__pb2\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_base_pb2 \u001b[38;5;28;01mas\u001b[39;00m wandb_dot_proto_dot_wandb__base__pb2\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_telemetry_pb2 \u001b[38;5;28;01mas\u001b[39;00m wandb_dot_proto_dot_wandb__telemetry__pb2\n\u001b[1;32m     19\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[1;32m     20\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb/proto/wandb_internal.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_internal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m   ,\n\u001b[1;32m     27\u001b[0m   dependencies\u001b[38;5;241m=\u001b[39m[google_dot_protobuf_dot_timestamp__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,wandb_dot_proto_dot_wandb__base__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,wandb_dot_proto_dot_wandb__telemetry__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,])\n",
            "File \u001b[0;32m~/.conda/envs/jupyter_env_311/lib/python3.11/site-packages/wandb/proto/wandb_base_pb2.py:36\u001b[0m\n\u001b[1;32m     11\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m     16\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[1;32m     17\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb/proto/wandb_base.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_internal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m   serialized_pb\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x1c\u001b[39;00m\u001b[38;5;124mwandb/proto/wandb_base.proto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0e\u001b[39;00m\u001b[38;5;124mwandb_internal\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m6\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x0b\u001b[39;00m\u001b[38;5;124m_RecordInfo\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x11\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mstream_id\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x14\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;124m_tracelog_id\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x64\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;124m_RequestInfo\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x11\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mstream_id\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m#\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x0b\u001b[39;00m\u001b[38;5;124m_ResultInfo\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x14\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x0...\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     28\u001b[0m __RECORDINFO \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[1;32m     29\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_RecordInfo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_internal._RecordInfo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m   filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m   file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     33\u001b[0m   containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m   create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key,\n\u001b[1;32m     35\u001b[0m   fields\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m---> 36\u001b[0m     \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFieldDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwandb_internal._RecordInfo.stream_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhas_default_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessage_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menum_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontaining_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43mis_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDESCRIPTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mcreate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_create_key\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     43\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mFieldDescriptor(\n\u001b[1;32m     44\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_tracelog_id\u001b[39m\u001b[38;5;124m'\u001b[39m, full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_internal._RecordInfo._tracelog_id\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     45\u001b[0m       number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, cpp_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     46\u001b[0m       has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     47\u001b[0m       message_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, enum_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m       is_extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, extension_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,  create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key),\n\u001b[1;32m     50\u001b[0m   ],\n\u001b[1;32m     51\u001b[0m   extensions\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     52\u001b[0m   ],\n\u001b[1;32m     53\u001b[0m   nested_types\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     54\u001b[0m   enum_types\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     55\u001b[0m   ],\n\u001b[1;32m     56\u001b[0m   serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m   is_extendable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m   syntax\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     59\u001b[0m   extension_ranges\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     60\u001b[0m   oneofs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     61\u001b[0m   ],\n\u001b[1;32m     62\u001b[0m   serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m,\n\u001b[1;32m     63\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m102\u001b[39m,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     67\u001b[0m __REQUESTINFO \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[1;32m     68\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_RequestInfo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     69\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_internal._RequestInfo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m137\u001b[39m,\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     99\u001b[0m __RESULTINFO \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[1;32m    100\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_ResultInfo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    101\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_internal._ResultInfo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m174\u001b[39m,\n\u001b[1;32m    128\u001b[0m )\n",
            "File \u001b[0;32m~/.conda/envs/jupyter_env_311/lib/python3.11/site-packages/google/protobuf/descriptor.py:621\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, full_name, index, number, \u001b[38;5;28mtype\u001b[39m, cpp_type, label,\n\u001b[1;32m    616\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[1;32m    617\u001b[0m             is_extension, extension_scope, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    618\u001b[0m             serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    619\u001b[0m             has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, containing_oneof\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    620\u001b[0m             file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m--> 621\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_extension:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _message\u001b[38;5;241m.\u001b[39mdefault_pool\u001b[38;5;241m.\u001b[39mFindExtensionByName(full_name)\n",
            "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
          ]
        }
      ]
    }
  ]
}