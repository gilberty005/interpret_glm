{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgW523C7mwLc"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWvH7kADmuf6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seorEmrxm6kD"
      },
      "outputs": [],
      "source": [
        "def create_file(dir: str, file_name: str) -> None:\n",
        "    if not os.path.isdir(dir):\n",
        "        raise ValueError(f\"The specified directory '{dir}' does not exist.\")\n",
        "\n",
        "    file_path = os.path.join(dir, file_name)\n",
        "    try:\n",
        "        open(file_path, \"x\").close()\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "\n",
        "\n",
        "def get_layer_activations(\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    plm: PreTrainedModel,\n",
        "    seqs: list[str],\n",
        "    layer: int,\n",
        "    device: Optional[torch.device] = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Get the activations of a specific layer in a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: The tokenizer to use.\n",
        "        plm: The pre-trained model to get activations from.\n",
        "        seqs: The list of sequences.\n",
        "        layer: The layer index to get activations from (0-based).\n",
        "        device: The device to use.\n",
        "\n",
        "    Returns:\n",
        "        The tensor of activations for the specified layer.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    inputs = tokenizer(seqs, padding=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = plm(**inputs, output_hidden_states=True)\n",
        "        if not hasattr(outputs, 'hidden_states') or not outputs.hidden_states:\n",
        "            raise ValueError(\"Model did not return hidden states.\")\n",
        "        layer_acts = outputs.hidden_states[layer]\n",
        "\n",
        "    return layer_acts\n",
        "\n",
        "\n",
        "def train_val_test_split(\n",
        "    df: pd.DataFrame, train_frac: float = 0.9\n",
        ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split the sequences into training, validation, and test sets. train_frac specifies\n",
        "    the fraction of examples to use for training; the rest is split evenly between\n",
        "    validation and test.\n",
        "\n",
        "    Doing this by samples so it's stochastic.\n",
        "\n",
        "    Args:\n",
        "        seqs: The sequences to split.\n",
        "        train_frac: The fraction of examples to use for training.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    is_train = pd.Series(\n",
        "        np.random.choice([True, False], size=len(df), p=[train_frac, 1 - train_frac])\n",
        "    )\n",
        "    seqs_train = df[is_train]\n",
        "    seqs_val_test = df[~is_train].reset_index(drop=True)\n",
        "\n",
        "    is_val = pd.Series(np.random.choice([True, False], size=len(seqs_val_test), p=[0.1, 0.9]))\n",
        "    seqs_val = seqs_val_test[is_val]\n",
        "    seqs_test = seqs_val_test[~is_val]\n",
        "    return seqs_train, seqs_val, seqs_test\n",
        "\n",
        "\n",
        "def parse_swissprot_annotation(annotation_str: str, header: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Parse a SwissProt annotation string like this:\n",
        "\n",
        "    ```\n",
        "    MOTIF 119..132; /note=\"JAMM motif\"; /evidence=\"ECO:0000255|PROSITE-ProRule:PRU01182\"\n",
        "    ```\n",
        "    where MOTIF is the header argument.\n",
        "\n",
        "    Returns:\n",
        "        [{\n",
        "            \"start\": 119,\n",
        "            \"end\": 132,\n",
        "            \"note\": \"JAMM motif\",\n",
        "            \"evidence\": \"ECO:0000255|PROSITE-ProRule:PRU01182\",\n",
        "        }]\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    occurrences = [o for o in annotation_str.split(header + \" \") if len(o) > 0]\n",
        "    for o in occurrences:\n",
        "        parts = [p for p in o.split(\"; /\") if len(p) > 0]\n",
        "\n",
        "        pos_part = parts[0]\n",
        "        coords = pos_part.split(\"..\")\n",
        "\n",
        "        annotations_dict = {}\n",
        "        for part in parts[1:]:\n",
        "            key, value = part.split(\"=\", 1)\n",
        "            annotations_dict[key] = value.replace('\"', \"\").replace(\";\", \"\").strip()\n",
        "\n",
        "        try:\n",
        "            list(map(int, coords))\n",
        "        except ValueError:\n",
        "            continue\n",
        "        if len(annotations_dict) == 0:\n",
        "            continue\n",
        "\n",
        "        res.append(\n",
        "            {\n",
        "                \"start\": int(coords[0]),\n",
        "                \"end\": int(coords[1]) if len(coords) > 1 else int(coords[0]),\n",
        "                **annotations_dict,\n",
        "            }\n",
        "        )\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df0KBE_zdQR7"
      },
      "source": [
        "#Data Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V47BXfZifPg8",
        "outputId": "2a715bd4-e301-40b8-c413-cc66583bc228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.6)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.10.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.5.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.8)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.10.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prMuUgVNdR65"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class PolarsDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        return {\"Sequence\": row[\"sequence\"], \"Entry\": row[\"id\"]}\n",
        "\n",
        "\n",
        "# Data Module\n",
        "class SequenceDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_path, batch_size):\n",
        "        super().__init__()\n",
        "        self.data_path = data_path\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        df = pd.read_parquet(self.data_path)\n",
        "        self.train_data, self.val_data, self.test_data = train_val_test_split(df)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            PolarsDataset(self.train_data), batch_size=self.batch_size, shuffle=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(PolarsDataset(self.val_data), batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            PolarsDataset(self.test_data), batch_size=self.batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq2QtNU8dgYm"
      },
      "source": [
        "#SAE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTqY6JDydiNE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
        "\n",
        "\n",
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        d_hidden: int,\n",
        "        k: int = 128,\n",
        "        auxk: int = 256,\n",
        "        batch_size: int = 256,\n",
        "        dead_steps_threshold: int = 2000,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the Sparse Autoencoder.\n",
        "\n",
        "        Args:\n",
        "            d_model: Dimension of the pLM model.\n",
        "            d_hidden: Dimension of the SAE hidden layer.\n",
        "            k: Number of top-k activations to keep.\n",
        "            auxk: Number of auxiliary activations.\n",
        "            dead_steps_threshold: How many examples of inactivation before we consider\n",
        "                a hidden dim dead.\n",
        "\n",
        "        Adapted from https://github.com/tylercosgrove/sparse-autoencoder-mistral7b/blob/main/sae.py\n",
        "        based on 'Scaling and evaluating sparse autoencoders' (Gao et al. 2024) https://arxiv.org/pdf/2406.04093\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_enc = nn.Parameter(torch.empty(d_model, d_hidden))\n",
        "        self.w_dec = nn.Parameter(torch.empty(d_hidden, d_model))\n",
        "\n",
        "        self.b_enc = nn.Parameter(torch.zeros(d_hidden))\n",
        "        self.b_pre = nn.Parameter(torch.zeros(d_model))\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        self.k = k\n",
        "        self.auxk = auxk\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.dead_steps_threshold = dead_steps_threshold / batch_size\n",
        "\n",
        "        # TODO: Revisit to see if this is the best way to initialize\n",
        "        nn.init.kaiming_uniform_(self.w_enc, a=math.sqrt(5))\n",
        "        self.w_dec.data = self.w_enc.data.T.clone()\n",
        "        self.w_dec.data /= self.w_dec.data.norm(dim=0)\n",
        "\n",
        "        # Initialize dead neuron tracking. For each hidden dimension, save the\n",
        "        # index of the example at which it was last activated.\n",
        "        self.register_buffer(\"stats_last_nonzero\", torch.zeros(d_hidden, dtype=torch.long))\n",
        "\n",
        "    def topK_activation(self, x: torch.Tensor, k: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply top-k activation to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to apply top-k activation on.\n",
        "            k: Number of top activations to keep.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor with only the top k activations preserved,and others\n",
        "            set to zero.\n",
        "\n",
        "        This function performs the following steps:\n",
        "        1. Find the top k values and their indices in the input tensor.\n",
        "        2. Apply ReLU activation to these top k values.\n",
        "        3. Create a new tensor of zeros with the same shape as the input.\n",
        "        4. Scatter the activated top k values back into their original positions.\n",
        "        \"\"\"\n",
        "        topk = torch.topk(x, k=k, dim=-1, sorted=False)\n",
        "        values = F.relu(topk.values)\n",
        "        result = torch.zeros_like(x)\n",
        "        result.scatter_(-1, topk.indices, values)\n",
        "        return result\n",
        "\n",
        "    def LN(\n",
        "        self, x: torch.Tensor, eps: float = 1e-5\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Apply Layer Normalization to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor to be normalized.\n",
        "            eps: A small value added to the denominator for numerical stability.\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "                - The normalized tensor.\n",
        "                - The mean of the input tensor.\n",
        "                - The standard deviation of the input tensor.\n",
        "\n",
        "        TODO: Is eps = 1e-5 the best value?\n",
        "        \"\"\"\n",
        "        mu = x.mean(dim=-1, keepdim=True)\n",
        "        x = x - mu\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        x = x / (std + eps)\n",
        "        return x, mu, std\n",
        "\n",
        "    def auxk_mask_fn(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Create a mask for dead neurons.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A boolean tensor of shape (D_HIDDEN,) where True indicates\n",
        "                a dead neuron.\n",
        "        \"\"\"\n",
        "        dead_mask = self.stats_last_nonzero > self.dead_steps_threshold\n",
        "        return dead_mask\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the Sparse Autoencoder. If there are dead neurons, compute the\n",
        "        reconstruction using the AUXK auxiliary hidden dims as well.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "                - The reconstructed activations via top K hidden dims.\n",
        "                - If there are dead neurons, the auxiliary activations via top AUXK\n",
        "                    hidden dims; otherwise, None.\n",
        "                - The number of dead neurons.\n",
        "        \"\"\"\n",
        "        x, mu, std = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "\n",
        "        pre_acts = x @ self.w_enc + self.b_enc\n",
        "\n",
        "        # latents: (BATCH_SIZE, D_EMBED, D_HIDDEN)\n",
        "        latents = self.topK_activation(pre_acts, k=self.k)\n",
        "\n",
        "        # `(latents == 0)` creates a boolean tensor element-wise from `latents`.\n",
        "        # `.all(dim=(0, 1))` preserves D_HIDDEN and does the boolean `all`\n",
        "        # operation across BATCH_SIZE and D_EMBED. Finally, `.long()` turns\n",
        "        # it into a vector of 0s and 1s of length D_HIDDEN.\n",
        "        #\n",
        "        # self.stats_last_nonzero is a vector of length D_HIDDEN. Doing\n",
        "        # `*=` with `M = (latents == 0).all(dim=(0, 1)).long()` has the effect\n",
        "        # of: if M[i] = 0, self.stats_last_nonzero[i] is cleared to 0, and then\n",
        "        # immediately incremented; if M[i] = 1, self.stats_last_nonzero[i] is\n",
        "        # unchanged. self.stats_last_nonzero[i] means \"for how many consecutive\n",
        "        # iterations has hidden dim i been zero\".\n",
        "        self.stats_last_nonzero *= (latents == 0).all(dim=(0, 1)).long()\n",
        "        self.stats_last_nonzero += 1\n",
        "\n",
        "        dead_mask = self.auxk_mask_fn()\n",
        "        num_dead = dead_mask.sum().item()\n",
        "\n",
        "        recons = latents @ self.w_dec + self.b_pre\n",
        "        recons = recons * std + mu\n",
        "\n",
        "        if num_dead > 0:\n",
        "            k_aux = min(x.shape[-1] // 2, num_dead)\n",
        "\n",
        "            auxk_latents = torch.where(dead_mask[None], pre_acts, -torch.inf)\n",
        "            auxk_acts = self.topK_activation(auxk_latents, k=k_aux)\n",
        "\n",
        "            auxk = auxk_acts @ self.w_dec + self.b_pre\n",
        "            auxk = auxk * std + mu\n",
        "        else:\n",
        "            auxk = None\n",
        "\n",
        "        return recons, auxk, num_dead\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward_val(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the Sparse Autoencoder for validation.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The reconstructed activations via top K hidden dims.\n",
        "        \"\"\"\n",
        "        x, mu, std = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "        pre_acts = x @ self.w_enc + self.b_enc\n",
        "        latents = self.topK_activation(pre_acts, self.k)\n",
        "\n",
        "        recons = latents @ self.w_dec + self.b_pre\n",
        "        recons = recons * std + mu\n",
        "        return recons\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def norm_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Normalize the weights of the Sparse Autoencoder.\n",
        "        \"\"\"\n",
        "        self.w_dec.data /= self.w_dec.data.norm(dim=0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def norm_grad(self) -> None:\n",
        "        \"\"\"\n",
        "        Normalize the gradient of the weights of the Sparse Autoencoder.\n",
        "        \"\"\"\n",
        "        dot_products = torch.sum(self.w_dec.data * self.w_dec.grad, dim=0)\n",
        "        self.w_dec.grad.sub_(self.w_dec.data * dot_products.unsqueeze(0))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_acts(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Get the activations of the Sparse Autoencoder.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The activations of the Sparse Autoencoder.\n",
        "        \"\"\"\n",
        "        x, _, _ = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "        pre_acts = x @ self.w_enc + self.b_enc\n",
        "        latents = self.topK_activation(pre_acts, self.k)\n",
        "        return latents\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x, mu, std = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "        acts = x @ self.w_enc + self.b_enc\n",
        "        return acts, mu, std\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def decode(self, acts: torch.Tensor, mu: torch.Tensor, std: torch.Tensor) -> torch.Tensor:\n",
        "        latents = self.topK_activation(acts, self.k)\n",
        "\n",
        "        recons = latents @ self.w_dec + self.b_pre\n",
        "        recons = recons * std + mu\n",
        "        return recons\n",
        "\n",
        "\n",
        "def loss_fn(\n",
        "    x: torch.Tensor, recons: torch.Tensor, auxk: Optional[torch.Tensor] = None\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Compute the loss function for the Sparse Autoencoder.\n",
        "\n",
        "    Args:\n",
        "        x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "        recons: (BATCH_SIZE, D_EMBED, D_MODEL) reconstructed activations via top K\n",
        "            hidden dims.\n",
        "        auxk: (BATCH_SIZE, D_EMBED, D_MODEL) auxiliary activations via top AUXK\n",
        "            hidden dims. See A.2. in https://arxiv.org/pdf/2406.04093.\n",
        "\n",
        "    Returns:\n",
        "        tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "            - The MSE loss.\n",
        "            - The auxiliary loss.\n",
        "    \"\"\"\n",
        "    mse_scale = 1\n",
        "    auxk_coeff = 1.0 / 32.0  # TODO: Is this the best coefficient?\n",
        "\n",
        "    mse_loss = mse_scale * F.mse_loss(recons, x)\n",
        "    if auxk is not None:\n",
        "        auxk_loss = auxk_coeff * F.mse_loss(auxk, x - recons).nan_to_num(0)\n",
        "    else:\n",
        "        auxk_loss = torch.tensor(0.0)\n",
        "    return mse_loss, auxk_loss\n",
        "\n",
        "\n",
        "def estimate_loss(\n",
        "    plm: PreTrainedModel,\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    layer: int,\n",
        "    sae_model: SparseAutoencoder,\n",
        "    examples_set: pd.DataFrame,\n",
        "    sample_size: int = 100,\n",
        "):\n",
        "    \"\"\"\n",
        "    Estimate the loss of the Sparse Autoencoder using a set of examples.\n",
        "\n",
        "    Args:\n",
        "        sae_model: The Sparse Autoencoder model.\n",
        "        examples_set: The examples set to estimate the loss on.\n",
        "        sample_size: The number of examples to sample.\n",
        "\n",
        "    Returns:\n",
        "        float: The estimated loss.\n",
        "    \"\"\"\n",
        "    samples = examples_set.sample(sample_size)\n",
        "    test_losses = []\n",
        "    seqs = [row[\"Sequence\"] for _, row in samples.iterrows()]\n",
        "    layer_acts = get_layer_activations(tokenizer=tokenizer, plm=plm, seqs=seqs, layer=layer)\n",
        "\n",
        "    recons = sae_model.forward_val(layer_acts)\n",
        "    mse_loss, _ = loss_fn(layer_acts, recons)\n",
        "    test_losses.append(mse_loss.item())\n",
        "\n",
        "    del layer_acts\n",
        "    return np.mean(test_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHHveI3OdmVY"
      },
      "source": [
        "#ESM Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "n4x1FcdRfW9y",
        "outputId": "80d4b29f-0d1e-4967-a957-02a08c34110a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: esm in /usr/local/lib/python3.10/dist-packages (3.0.7.post1)\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from esm) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from esm) (0.20.0+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (from esm) (0.18.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from esm) (4.44.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from esm) (7.34.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from esm) (0.8.0)\n",
            "Requirement already satisfied: biotite==0.41.2 in /usr/local/lib/python3.10/dist-packages (from esm) (0.41.2)\n",
            "Requirement already satisfied: msgpack-numpy in /usr/local/lib/python3.10/dist-packages (from esm) (0.4.8)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (from esm) (1.84)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from esm) (1.5.2)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.10/dist-packages (from esm) (1.1.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from esm) (24.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from esm) (2.2.2)\n",
            "Requirement already satisfied: cloudpathlib in /usr/local/lib/python3.10/dist-packages (from esm) (0.20.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from esm) (9.0.0)\n",
            "Requirement already satisfied: msgpack>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from biotite==0.41.2->esm) (1.1.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from biotite==0.41.2->esm) (3.4.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from biotite==0.41.2->esm) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.12 in /usr/local/lib/python3.10/dist-packages (from biotite==0.41.2->esm) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->esm) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->esm) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->esm) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->esm) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->esm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.2.0->esm) (1.3.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->esm) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->esm) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->esm) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->esm) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->esm) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->esm) (3.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext->esm) (4.66.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->esm) (10.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->esm) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->esm) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->esm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->esm) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->esm) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->esm) (0.19.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->esm) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->esm) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->esm) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->esm) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite==0.41.2->esm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite==0.41.2->esm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite==0.41.2->esm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite==0.41.2->esm) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.2.0->esm) (3.0.2)\n",
            "Requirement already satisfied: fair-esm in /usr/local/lib/python3.10/dist-packages (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install esm\n",
        "!pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw5jHz3udzLB"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from esm.modules import ESM1bLayerNorm, RobertaLMHead, TransformerLayer\n",
        "\n",
        "class ESM2Model(pl.LightningModule):\n",
        "    def __init__(self, num_layers, embed_dim, attention_heads, alphabet, token_dropout):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.embed_dim = embed_dim\n",
        "        self.attention_heads = attention_heads\n",
        "        self.alphabet = alphabet\n",
        "        self.alphabet_size = len(alphabet)\n",
        "        self.batch_converter = self.alphabet.get_batch_converter()\n",
        "        self.padding_idx = alphabet.padding_idx\n",
        "        self.mask_idx = alphabet.mask_idx\n",
        "        self.cls_idx = alphabet.cls_idx\n",
        "        self.eos_idx = alphabet.eos_idx\n",
        "        self.prepend_bos = alphabet.prepend_bos\n",
        "        self.append_eos = alphabet.append_eos\n",
        "        self.token_dropout = token_dropout\n",
        "        self._init_submodules()\n",
        "\n",
        "    def _init_submodules(self):\n",
        "        self.embed_scale = 1\n",
        "        self.embed_tokens = nn.Embedding(\n",
        "            self.alphabet_size,\n",
        "            self.embed_dim,\n",
        "            padding_idx=self.padding_idx,\n",
        "        )\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerLayer(\n",
        "                    self.embed_dim,\n",
        "                    4 * self.embed_dim,\n",
        "                    self.attention_heads,\n",
        "                    add_bias_kv=False,\n",
        "                    use_esm1b_layer_norm=True,\n",
        "                    use_rotary_embeddings=True,\n",
        "                )\n",
        "                for _ in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.emb_layer_norm_after = ESM1bLayerNorm(self.embed_dim)\n",
        "\n",
        "        self.lm_head = RobertaLMHead(\n",
        "            embed_dim=self.embed_dim,\n",
        "            output_dim=self.alphabet_size,\n",
        "            weight=self.embed_tokens.weight,\n",
        "        )\n",
        "\n",
        "    def load_esm_ckpt(self, esm_pretrained):\n",
        "        ckpt = {}\n",
        "        model_data = torch.load(esm_pretrained)[\"model\"]\n",
        "        for k in model_data:\n",
        "            if \"lm_head\" in k:\n",
        "                ckpt[k.replace(\"encoder.\", \"\")] = model_data[k]\n",
        "            else:\n",
        "                ckpt[k.replace(\"encoder.sentence_encoder.\", \"\")] = model_data[k]\n",
        "        self.load_state_dict(ckpt)\n",
        "\n",
        "    def compose_input(self, list_tuple_seq):\n",
        "      _, _, batch_tokens = self.batch_converter(list_tuple_seq)\n",
        "      device = next(self.parameters()).device\n",
        "      batch_tokens = batch_tokens.to(device)\n",
        "      return batch_tokens\n",
        "\n",
        "    def forward(self, tokens, output_hidden_states=False):\n",
        "        # Embedding\n",
        "        x = self.embed_scale * self.embed_tokens(tokens)\n",
        "\n",
        "        # Pass through layers\n",
        "        hidden_states = []\n",
        "        x = x.transpose(0, 1)  # (B, T, E) -> (T, B, E)\n",
        "        for layer in self.layers:\n",
        "            x, _ = layer(x)\n",
        "            if output_hidden_states:\n",
        "                hidden_states.append(x.transpose(0, 1))  # Store (B, T, E)\n",
        "\n",
        "        x = x.transpose(0, 1)  # (T, B, E) -> (B, T, E)\n",
        "        x = self.emb_layer_norm_after(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        outputs = {'logits': logits}\n",
        "        if output_hidden_states:\n",
        "            outputs['hidden_states'] = hidden_states\n",
        "        return outputs\n",
        "\n",
        "    def get_layer_activations(self, input, layer_idx):\n",
        "      if isinstance(input, str):\n",
        "        tokens = self.compose_input([(\"protein\", input)])\n",
        "      elif isinstance(input, list):\n",
        "        tokens = self.compose_input([(\"protein\", seq) for seq in input])\n",
        "      else:\n",
        "        tokens = input\n",
        "\n",
        "      # Get the attention mask if needed\n",
        "      attention_mask = tokens.ne(self.padding_idx)\n",
        "\n",
        "      x = self.embed_scale * self.embed_tokens(tokens)\n",
        "      x = x.transpose(0, 1)  # (B, T, E) => (T, B, E)\n",
        "\n",
        "      # Process up to the desired layer\n",
        "      for idx, layer in enumerate(self.layers):\n",
        "          x, _ = layer(x, self_attn_padding_mask=attention_mask)\n",
        "          if idx == layer_idx - 1:\n",
        "              break\n",
        "\n",
        "      x = x.transpose(0, 1)  # (T, B, E) => (B, T, E)\n",
        "      return tokens, x\n",
        "\n",
        "    def get_sequence(self, x, layer_idx):\n",
        "        x = x.transpose(0, 1)  # (B, T, E) => (T, B, E)\n",
        "        for _, layer in enumerate(self.layers[layer_idx:]):\n",
        "            x, attn = layer(\n",
        "                x,\n",
        "                self_attn_padding_mask=None,\n",
        "                need_head_weights=False,\n",
        "            )\n",
        "        x = self.emb_layer_norm_after(x)\n",
        "        x = x.transpose(0, 1)  # (T, B, E) => (B, T, E)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZMtsjGXmzyL"
      },
      "source": [
        "#SAE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2suV2Z2Se1tp"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFa4MgSRnAzB"
      },
      "outputs": [],
      "source": [
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        d_hidden: int,\n",
        "        k: int = 128,\n",
        "        auxk: int = 256,\n",
        "        batch_size: int = 256,\n",
        "        dead_steps_threshold: int = 2000,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the Sparse Autoencoder.\n",
        "\n",
        "        Args:\n",
        "            d_model: Dimension of the pLM model.\n",
        "            d_hidden: Dimension of the SAE hidden layer.\n",
        "            k: Number of top-k activations to keep.\n",
        "            auxk: Number of auxiliary activations.\n",
        "            dead_steps_threshold: How many examples of inactivation before we consider\n",
        "                a hidden dim dead.\n",
        "\n",
        "        Adapted from https://github.com/tylercosgrove/sparse-autoencoder-mistral7b/blob/main/sae.py\n",
        "        based on 'Scaling and evaluating sparse autoencoders' (Gao et al. 2024) https://arxiv.org/pdf/2406.04093\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_enc = nn.Parameter(torch.empty(d_model, d_hidden))\n",
        "        self.w_dec = nn.Parameter(torch.empty(d_hidden, d_model))\n",
        "\n",
        "        self.b_enc = nn.Parameter(torch.zeros(d_hidden))\n",
        "        self.b_pre = nn.Parameter(torch.zeros(d_model))\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        self.k = k\n",
        "        self.auxk = auxk\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.dead_steps_threshold = dead_steps_threshold / batch_size\n",
        "\n",
        "        # TODO: Revisit to see if this is the best way to initialize\n",
        "        nn.init.kaiming_uniform_(self.w_enc, a=math.sqrt(5))\n",
        "        self.w_dec.data = self.w_enc.data.T.clone()\n",
        "        self.w_dec.data /= self.w_dec.data.norm(dim=0)\n",
        "\n",
        "        # Initialize dead neuron tracking. For each hidden dimension, save the\n",
        "        # index of the example at which it was last activated.\n",
        "        self.register_buffer(\"stats_last_nonzero\", torch.zeros(d_hidden, dtype=torch.long))\n",
        "\n",
        "    def topK_activation(self, x: torch.Tensor, k: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply top-k activation to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to apply top-k activation on.\n",
        "            k: Number of top activations to keep.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor with only the top k activations preserved,and others\n",
        "            set to zero.\n",
        "\n",
        "        This function performs the following steps:\n",
        "        1. Find the top k values and their indices in the input tensor.\n",
        "        2. Apply ReLU activation to these top k values.\n",
        "        3. Create a new tensor of zeros with the same shape as the input.\n",
        "        4. Scatter the activated top k values back into their original positions.\n",
        "        \"\"\"\n",
        "        topk = torch.topk(x, k=k, dim=-1, sorted=False)\n",
        "        values = F.relu(topk.values)\n",
        "        result = torch.zeros_like(x)\n",
        "        result.scatter_(-1, topk.indices, values)\n",
        "        return result\n",
        "\n",
        "    def LN(\n",
        "        self, x: torch.Tensor, eps: float = 1e-5\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Apply Layer Normalization to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor to be normalized.\n",
        "            eps: A small value added to the denominator for numerical stability.\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "                - The normalized tensor.\n",
        "                - The mean of the input tensor.\n",
        "                - The standard deviation of the input tensor.\n",
        "\n",
        "        TODO: Is eps = 1e-5 the best value?\n",
        "        \"\"\"\n",
        "        mu = x.mean(dim=-1, keepdim=True)\n",
        "        x = x - mu\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        x = x / (std + eps)\n",
        "        return x, mu, std\n",
        "\n",
        "    def auxk_mask_fn(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Create a mask for dead neurons.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A boolean tensor of shape (D_HIDDEN,) where True indicates\n",
        "                a dead neuron.\n",
        "        \"\"\"\n",
        "        dead_mask = self.stats_last_nonzero > self.dead_steps_threshold\n",
        "        return dead_mask\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the Sparse Autoencoder. If there are dead neurons, compute the\n",
        "        reconstruction using the AUXK auxiliary hidden dims as well.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "                - The reconstructed activations via top K hidden dims.\n",
        "                - If there are dead neurons, the auxiliary activations via top AUXK\n",
        "                    hidden dims; otherwise, None.\n",
        "                - The number of dead neurons.\n",
        "        \"\"\"\n",
        "        x, mu, std = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "\n",
        "        pre_acts = x @ self.w_enc + self.b_enc\n",
        "\n",
        "        # latents: (BATCH_SIZE, D_EMBED, D_HIDDEN)\n",
        "        latents = self.topK_activation(pre_acts, k=self.k)\n",
        "\n",
        "        # `(latents == 0)` creates a boolean tensor element-wise from `latents`.\n",
        "        # `.all(dim=(0, 1))` preserves D_HIDDEN and does the boolean `all`\n",
        "        # operation across BATCH_SIZE and D_EMBED. Finally, `.long()` turns\n",
        "        # it into a vector of 0s and 1s of length D_HIDDEN.\n",
        "        #\n",
        "        # self.stats_last_nonzero is a vector of length D_HIDDEN. Doing\n",
        "        # `*=` with `M = (latents == 0).all(dim=(0, 1)).long()` has the effect\n",
        "        # of: if M[i] = 0, self.stats_last_nonzero[i] is cleared to 0, and then\n",
        "        # immediately incremented; if M[i] = 1, self.stats_last_nonzero[i] is\n",
        "        # unchanged. self.stats_last_nonzero[i] means \"for how many consecutive\n",
        "        # iterations has hidden dim i been zero\".\n",
        "        self.stats_last_nonzero *= (latents == 0).all(dim=(0, 1)).long()\n",
        "        self.stats_last_nonzero += 1\n",
        "\n",
        "        dead_mask = self.auxk_mask_fn()\n",
        "        num_dead = dead_mask.sum().item()\n",
        "\n",
        "        recons = latents @ self.w_dec + self.b_pre\n",
        "        recons = recons * std + mu\n",
        "\n",
        "        if num_dead > 0:\n",
        "            k_aux = min(x.shape[-1] // 2, num_dead)\n",
        "\n",
        "            auxk_latents = torch.where(dead_mask[None], pre_acts, -torch.inf)\n",
        "            auxk_acts = self.topK_activation(auxk_latents, k=k_aux)\n",
        "\n",
        "            auxk = auxk_acts @ self.w_dec + self.b_pre\n",
        "            auxk = auxk * std + mu\n",
        "        else:\n",
        "            auxk = None\n",
        "\n",
        "        return recons, auxk, num_dead\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward_val(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the Sparse Autoencoder for validation.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The reconstructed activations via top K hidden dims.\n",
        "        \"\"\"\n",
        "        x, mu, std = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "        pre_acts = x @ self.w_enc + self.b_enc\n",
        "        latents = self.topK_activation(pre_acts, self.k)\n",
        "\n",
        "        recons = latents @ self.w_dec + self.b_pre\n",
        "        recons = recons * std + mu\n",
        "        return recons\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def norm_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Normalize the weights of the Sparse Autoencoder.\n",
        "        \"\"\"\n",
        "        self.w_dec.data /= self.w_dec.data.norm(dim=0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def norm_grad(self) -> None:\n",
        "        \"\"\"\n",
        "        Normalize the gradient of the weights of the Sparse Autoencoder.\n",
        "        \"\"\"\n",
        "        dot_products = torch.sum(self.w_dec.data * self.w_dec.grad, dim=0)\n",
        "        self.w_dec.grad.sub_(self.w_dec.data * dot_products.unsqueeze(0))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_acts(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Get the activations of the Sparse Autoencoder.\n",
        "\n",
        "        Args:\n",
        "            x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The activations of the Sparse Autoencoder.\n",
        "        \"\"\"\n",
        "        x, _, _ = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "        pre_acts = x @ self.w_enc + self.b_enc\n",
        "        latents = self.topK_activation(pre_acts, self.k)\n",
        "        return latents\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x, mu, std = self.LN(x)\n",
        "        x = x - self.b_pre\n",
        "        acts = x @ self.w_enc + self.b_enc\n",
        "        return acts, mu, std\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def decode(self, acts: torch.Tensor, mu: torch.Tensor, std: torch.Tensor) -> torch.Tensor:\n",
        "        latents = self.topK_activation(acts, self.k)\n",
        "\n",
        "        recons = latents @ self.w_dec + self.b_pre\n",
        "        recons = recons * std + mu\n",
        "        return recons\n",
        "\n",
        "\n",
        "def loss_fn(\n",
        "    x: torch.Tensor, recons: torch.Tensor, auxk: Optional[torch.Tensor] = None\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Compute the loss function for the Sparse Autoencoder.\n",
        "\n",
        "    Args:\n",
        "        x: (BATCH_SIZE, D_EMBED, D_MODEL) input tensor to the SAE.\n",
        "        recons: (BATCH_SIZE, D_EMBED, D_MODEL) reconstructed activations via top K\n",
        "            hidden dims.\n",
        "        auxk: (BATCH_SIZE, D_EMBED, D_MODEL) auxiliary activations via top AUXK\n",
        "            hidden dims. See A.2. in https://arxiv.org/pdf/2406.04093.\n",
        "\n",
        "    Returns:\n",
        "        tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
        "            - The MSE loss.\n",
        "            - The auxiliary loss.\n",
        "    \"\"\"\n",
        "    mse_scale = 1\n",
        "    auxk_coeff = 1.0 / 32.0  # TODO: Is this the best coefficient?\n",
        "\n",
        "    mse_loss = mse_scale * F.mse_loss(recons, x)\n",
        "    if auxk is not None:\n",
        "        auxk_loss = auxk_coeff * F.mse_loss(auxk, x - recons).nan_to_num(0)\n",
        "    else:\n",
        "        auxk_loss = torch.tensor(0.0)\n",
        "    return mse_loss, auxk_loss\n",
        "\n",
        "\n",
        "def estimate_loss(\n",
        "    plm: PreTrainedModel,\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    layer: int,\n",
        "    sae_model: SparseAutoencoder,\n",
        "    examples_set: pd.DataFrame,\n",
        "    sample_size: int = 100,\n",
        "):\n",
        "    \"\"\"\n",
        "    Estimate the loss of the Sparse Autoencoder using a set of examples.\n",
        "\n",
        "    Args:\n",
        "        sae_model: The Sparse Autoencoder model.\n",
        "        examples_set: The examples set to estimate the loss on.\n",
        "        sample_size: The number of examples to sample.\n",
        "\n",
        "    Returns:\n",
        "        float: The estimated loss.\n",
        "    \"\"\"\n",
        "    samples = examples_set.sample(sample_size)\n",
        "    test_losses = []\n",
        "    seqs = [row[\"Sequence\"] for row in samples.iter_rows(named=True)]\n",
        "    layer_acts = get_layer_activations(tokenizer=tokenizer, plm=plm, seqs=seqs, layer=layer)\n",
        "\n",
        "    recons = sae_model.forward_val(layer_acts)\n",
        "    mse_loss, _ = loss_fn(layer_acts, recons)\n",
        "    test_losses.append(mse_loss.item())\n",
        "\n",
        "    del layer_acts\n",
        "    return np.mean(test_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At8Di7ODgfNx"
      },
      "source": [
        "#SAE Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8xcCpjzgglC"
      },
      "outputs": [],
      "source": [
        "import esm\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_esm_model(d_model, alphabet, esm2_weight):\n",
        "    esm2_model = ESM2Model(\n",
        "        num_layers=33,\n",
        "        embed_dim=d_model,\n",
        "        attention_heads=20,\n",
        "        alphabet=alphabet,\n",
        "        token_dropout=False,\n",
        "    )\n",
        "    state_dict = torch.load(esm2_weight)\n",
        "    esm2_model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    esm2_model.eval()\n",
        "    for param in esm2_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    return esm2_model\n",
        "\n",
        "class SAELightningModule(pl.LightningModule):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.args = args\n",
        "        self.layer_to_use = args.layer_to_use\n",
        "        self.sae_model = SparseAutoencoder(\n",
        "            d_model=args.d_model,\n",
        "            d_hidden=args.d_hidden,\n",
        "            k=args.k,\n",
        "            auxk=args.auxk,\n",
        "            batch_size=args.batch_size,\n",
        "            dead_steps_threshold=args.dead_steps_threshold,\n",
        "        )\n",
        "        self.alphabet = esm.data.Alphabet.from_architecture(\"ESM-1b\")\n",
        "        self.esm2_model = get_esm_model(self.args.d_model, self.alphabet, self.args.esm2_weight)\n",
        "\n",
        "    def on_fit_start(self):\n",
        "        device = self.device\n",
        "        self.esm2_model.to(device)\n",
        "        self.sae_model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sae_model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        seqs = batch[\"Sequence\"]\n",
        "        batch_size = len(seqs)\n",
        "        with torch.no_grad():\n",
        "            tokens, esm_layer_acts = self.esm2_model.get_layer_activations(seqs, self.layer_to_use)\n",
        "\n",
        "        esm_layer_acts = esm_layer_acts.to(self.device)\n",
        "        recons, auxk, num_dead = self(esm_layer_acts)\n",
        "        mse_loss, auxk_loss = loss_fn(esm_layer_acts, recons, auxk)\n",
        "        loss = mse_loss + auxk_loss\n",
        "\n",
        "        self.log(\n",
        "            \"train_loss\",\n",
        "            loss,\n",
        "            on_step=True,\n",
        "            on_epoch=True,\n",
        "            prog_bar=True,\n",
        "            logger=True,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        self.log(\n",
        "            \"train_mse_loss\",\n",
        "            mse_loss,\n",
        "            on_step=True,\n",
        "            on_epoch=True,\n",
        "            logger=True,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        self.log(\n",
        "            \"train_auxk_loss\",\n",
        "            auxk_loss,\n",
        "            on_step=True,\n",
        "            on_epoch=True,\n",
        "            logger=True,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        self.log(\n",
        "            \"num_dead_neurons\",\n",
        "            num_dead,\n",
        "            on_step=True,\n",
        "            on_epoch=True,\n",
        "            logger=True,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        seqs = batch[\"Sequence\"]\n",
        "        batch_size = len(seqs)\n",
        "        with torch.no_grad():\n",
        "            tokens, esm_layer_acts = self.esm2_model.get_layer_activations(seqs, self.layer_to_use)\n",
        "            esm_layer_acts = esm_layer_acts.to(self.device)\n",
        "            recons, auxk, num_dead = self(esm_layer_acts)\n",
        "            mse_loss, auxk_loss = loss_fn(esm_layer_acts, recons, auxk)\n",
        "            loss = mse_loss + auxk_loss\n",
        "            logits = self.esm2_model.get_sequence(recons, self.layer_to_use)\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            tokens = tokens.view(-1)\n",
        "            correct = (torch.argmax(logits, dim=-1) == tokens).sum().item()\n",
        "            total = tokens.size(0)\n",
        "\n",
        "        self.log(\n",
        "            \"val_celoss\",\n",
        "            F.cross_entropy(logits, tokens).mean().item(),\n",
        "            on_step=True,\n",
        "            on_epoch=True,\n",
        "            prog_bar=True,\n",
        "            logger=True,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        self.log(\n",
        "            \"val_acc\",\n",
        "            correct / total,\n",
        "            on_step=True,\n",
        "            on_epoch=True,\n",
        "            prog_bar=True,\n",
        "            logger=True,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        self.log(\n",
        "            \"val_loss\",\n",
        "            loss,\n",
        "            on_step=True,\n",
        "            on_epoch=True,\n",
        "            prog_bar=True,\n",
        "            logger=True,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.validation_step(batch, batch_idx)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=self.args.lr)\n",
        "\n",
        "    def on_after_backward(self):\n",
        "        self.sae_model.norm_weights()\n",
        "        self.sae_model.norm_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IamEPGVSfhPY"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zn8oDSnnKf2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import pytorch_lightning as pl_lightning\n",
        "import wandb\n",
        "import numpy as np\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNn1bf7b2gFm"
      },
      "outputs": [],
      "source": [
        "model_weights, alphabet = esm.pretrained.esm2_t33_650M_UR50D()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAPhRum-LqPm",
        "outputId": "f041d477-2fe9-4fe0-e812-04775dce40ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXboJqgw4cx2",
        "outputId": "608ea8f4-f225-4154-9eeb-c73232134766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 604,
          "referenced_widgets": [
            "d79bbd742dbc47a4a5e4d29693ad5f4b",
            "b7c2001750404868bac429895ae83300",
            "933fe26529904154b1ae7d80e3032462",
            "dfd29ee6600a43c6b67d39ac99983499",
            "a9acfe67126c4d1bbb09307b8cb941c8",
            "8ccf2ed9769a4abc85c2b220bd31e184",
            "15b5e35e5c2a45108196f36a60bb158c",
            "f9742368972f4cf6b488bac66f986639",
            "2ad4de7de74a4a3aa7a3f8493bbb7301",
            "175748a653ec4df386e94154102fce5a",
            "0a710f6d0fd140b28fa109f5d64984b0",
            "253d2588988a411993155ccce1b96e80",
            "6dfe8065b8424bc99fddd752e691cedc",
            "729b11b74be4470884547054a53cf65f",
            "54143a5fe7a74c509149be8c6c682f36",
            "9e6960028cdb4e44a5511726390ec2e4",
            "d751b35e461d4aef93ea72aac37b5afa",
            "7e35d55df4e64b2e9a3ac97a77eafbdf",
            "fdd0919bc74d461ca6b610cd79f40ff9",
            "20621282d9cc4f019d44ace20c8118b0",
            "f8b879cf1059488ab44e3011818831f4",
            "c5f239c5e01147bf83265207bec17a0f",
            "87d99eea719547e4bee8415631878170",
            "971f4644903e47a89c1b5dba1df6e793",
            "e15e79c054864ef99132caf3d93b1604",
            "fa1d3533224a42619c122f60a923e955",
            "d36ce8b0ee984fe8ae79de127df57883",
            "076b1c2af2c54dcaac9733afb2d63653",
            "bead2ba73196471a833baf37ce1b649d",
            "68e1f0e678c3475f9655f0c465216146",
            "250ce9a0428b4846a09cd50f2cd1ade4",
            "f7abb4ae21d644ebb5dac56c78dd6057",
            "4e185130381d4a2283ffbd9aacd05bb8",
            "7c16ccf5e0b544b7b8752b9bcb1bc9c1"
          ]
        },
        "id": "X2HhP9sOgoYh",
        "outputId": "66d79fd2-c947-40b2-ff66-bf24c31596e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-d400adb04062>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(esm2_weight)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgilberty005\u001b[0m (\u001b[33mgilberty005-columbia-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path results_l24_dim32768_k128/wandb/wandb/ wasn't writable, using system temp directory.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/tmp/wandb/run-20241112_105736-gjm3olt7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gilberty005-columbia-university/interpretability/runs/gjm3olt7' target=\"_blank\">esm2_plm1280_l24_sae32768_k128_auxk256</a></strong> to <a href='https://wandb.ai/gilberty005-columbia-university/interpretability' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gilberty005-columbia-university/interpretability' target=\"_blank\">https://wandb.ai/gilberty005-columbia-university/interpretability</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gilberty005-columbia-university/interpretability/runs/gjm3olt7' target=\"_blank\">https://wandb.ai/gilberty005-columbia-university/interpretability/runs/gjm3olt7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /content/results_l24_dim32768_k128/checkpoints exists and is not empty.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name       | Type              | Params | Mode \n",
            "---------------------------------------------------------\n",
            "0 | sae_model  | SparseAutoencoder | 83.9 M | train\n",
            "1 | esm2_model | ESM2Model         | 651 M  | eval \n",
            "---------------------------------------------------------\n",
            "83.9 M    Trainable params\n",
            "651 M     Non-trainable params\n",
            "734 M     Total params\n",
            "2,939.851 Total estimated model params size (MB)\n",
            "1         Modules in train mode\n",
            "370       Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d79bbd742dbc47a4a5e4d29693ad5f4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "253d2588988a411993155ccce1b96e80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87d99eea719547e4bee8415631878170",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c16ccf5e0b544b7b8752b9bcb1bc9c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()\n",
        "\n",
        "# Define the arguments manually\n",
        "args = SimpleNamespace(\n",
        "    data_dir=\"drive/MyDrive/data_sae/uniref50_1M_1022.parquet\",\n",
        "    esm2_weight=\"drive/MyDrive/data_sae/esm2_t33_650M_UR50D.pt\",\n",
        "    #esm2_weight = \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\",\n",
        "    layer_to_use=24,\n",
        "    d_model=1280,\n",
        "    d_hidden=32768,\n",
        "    batch_size=4,\n",
        "    lr=2e-4,\n",
        "    k=128,\n",
        "    auxk=256,\n",
        "    dead_steps_threshold=2000,\n",
        "    max_epochs=4,\n",
        "    num_devices=1,\n",
        ")\n",
        "\n",
        "args.output_dir = f\"results_l{args.layer_to_use}_dim{args.d_hidden}_k{args.k}\"\n",
        "\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.mkdir(args.output_dir)\n",
        "\n",
        "sae_name = f\"esm2_plm1280_l{args.layer_to_use}_sae{args.d_hidden}_k{args.k}_auxk{args.auxk}\"\n",
        "\n",
        "wandb_logger = WandbLogger(\n",
        "    project=\"interpretability\",\n",
        "    name=sae_name,\n",
        "    save_dir=os.path.join(args.output_dir, \"wandb\"),\n",
        ")\n",
        "\n",
        "model = SAELightningModule(args)\n",
        "data_module = SequenceDataModule(args.data_dir, args.batch_size)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=os.path.join(args.output_dir, \"checkpoints\"),\n",
        "    filename=sae_name + \"-{step}-{val_loss:.2f}\",\n",
        "    save_top_k=3,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_last=True,\n",
        ")\n",
        "\n",
        "trainer = pl_lightning.Trainer(\n",
        "    max_epochs=args.max_epochs,\n",
        "    accelerator=\"gpu\",\n",
        "    devices=list(range(args.num_devices)),\n",
        "    strategy=\"ddp\" if args.num_devices > 1 else \"auto\",\n",
        "    logger=wandb_logger,\n",
        "    log_every_n_steps=10,\n",
        "    val_check_interval=2000,\n",
        "    limit_val_batches=10,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    gradient_clip_val=1.0,\n",
        ")\n",
        "\n",
        "trainer.fit(model, data_module)\n",
        "trainer.test(model, data_module)\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "076b1c2af2c54dcaac9733afb2d63653": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a710f6d0fd140b28fa109f5d64984b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15b5e35e5c2a45108196f36a60bb158c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "175748a653ec4df386e94154102fce5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20621282d9cc4f019d44ace20c8118b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "250ce9a0428b4846a09cd50f2cd1ade4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "253d2588988a411993155ccce1b96e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dfe8065b8424bc99fddd752e691cedc",
              "IPY_MODEL_729b11b74be4470884547054a53cf65f",
              "IPY_MODEL_54143a5fe7a74c509149be8c6c682f36"
            ],
            "layout": "IPY_MODEL_9e6960028cdb4e44a5511726390ec2e4"
          }
        },
        "2ad4de7de74a4a3aa7a3f8493bbb7301": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e185130381d4a2283ffbd9aacd05bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54143a5fe7a74c509149be8c6c682f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8b879cf1059488ab44e3011818831f4",
            "placeholder": "​",
            "style": "IPY_MODEL_c5f239c5e01147bf83265207bec17a0f",
            "value": " 2000/225128 [37:40&lt;70:03:42,  0.88it/s, v_num=olt7, train_loss_step=nan.0, val_celoss_step=nan.0, val_acc_step=0.00256, val_loss_step=nan.0, val_celoss_epoch=nan.0, val_acc_epoch=0.00312, val_loss_epoch=nan.0]"
          }
        },
        "68e1f0e678c3475f9655f0c465216146": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dfe8065b8424bc99fddd752e691cedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d751b35e461d4aef93ea72aac37b5afa",
            "placeholder": "​",
            "style": "IPY_MODEL_7e35d55df4e64b2e9a3ac97a77eafbdf",
            "value": "Epoch 0:   1%"
          }
        },
        "729b11b74be4470884547054a53cf65f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdd0919bc74d461ca6b610cd79f40ff9",
            "max": 225128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20621282d9cc4f019d44ace20c8118b0",
            "value": 2000
          }
        },
        "7e35d55df4e64b2e9a3ac97a77eafbdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87d99eea719547e4bee8415631878170": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_971f4644903e47a89c1b5dba1df6e793",
              "IPY_MODEL_e15e79c054864ef99132caf3d93b1604",
              "IPY_MODEL_fa1d3533224a42619c122f60a923e955"
            ],
            "layout": "IPY_MODEL_d36ce8b0ee984fe8ae79de127df57883"
          }
        },
        "8ccf2ed9769a4abc85c2b220bd31e184": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "933fe26529904154b1ae7d80e3032462": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9742368972f4cf6b488bac66f986639",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ad4de7de74a4a3aa7a3f8493bbb7301",
            "value": 2
          }
        },
        "971f4644903e47a89c1b5dba1df6e793": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_076b1c2af2c54dcaac9733afb2d63653",
            "placeholder": "​",
            "style": "IPY_MODEL_bead2ba73196471a833baf37ce1b649d",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "9e6960028cdb4e44a5511726390ec2e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "a9acfe67126c4d1bbb09307b8cb941c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "b7c2001750404868bac429895ae83300": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ccf2ed9769a4abc85c2b220bd31e184",
            "placeholder": "​",
            "style": "IPY_MODEL_15b5e35e5c2a45108196f36a60bb158c",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "bead2ba73196471a833baf37ce1b649d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5f239c5e01147bf83265207bec17a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d36ce8b0ee984fe8ae79de127df57883": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "d751b35e461d4aef93ea72aac37b5afa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d79bbd742dbc47a4a5e4d29693ad5f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7c2001750404868bac429895ae83300",
              "IPY_MODEL_933fe26529904154b1ae7d80e3032462",
              "IPY_MODEL_dfd29ee6600a43c6b67d39ac99983499"
            ],
            "layout": "IPY_MODEL_a9acfe67126c4d1bbb09307b8cb941c8"
          }
        },
        "dfd29ee6600a43c6b67d39ac99983499": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_175748a653ec4df386e94154102fce5a",
            "placeholder": "​",
            "style": "IPY_MODEL_0a710f6d0fd140b28fa109f5d64984b0",
            "value": " 2/2 [00:01&lt;00:00,  1.21it/s]"
          }
        },
        "e15e79c054864ef99132caf3d93b1604": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68e1f0e678c3475f9655f0c465216146",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_250ce9a0428b4846a09cd50f2cd1ade4",
            "value": 10
          }
        },
        "f7abb4ae21d644ebb5dac56c78dd6057": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b879cf1059488ab44e3011818831f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9742368972f4cf6b488bac66f986639": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa1d3533224a42619c122f60a923e955": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7abb4ae21d644ebb5dac56c78dd6057",
            "placeholder": "​",
            "style": "IPY_MODEL_4e185130381d4a2283ffbd9aacd05bb8",
            "value": " 10/10 [00:07&lt;00:00,  1.37it/s]"
          }
        },
        "fdd0919bc74d461ca6b610cd79f40ff9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}